todo:
- посмотреть tokenkit, какие у него возможности, может сможет работать с большим количеством токенизаторов, чем штука от меты.
- взять любую легкую модель, попытаться сконвертировать в битовый формат (напр, через токенкит, генлм, может еще какие-то способы оптимальные есть).
- накидать идеи, куда можно от этого двинуться, может можно придумать как делать конвершн в битовый формат чуть быстрее либо за счет построения КА элементами из токенизатора (BPE tokenizer).
- посмотреть, что получается, если смотрим на likelihood побитовых последовательностей (посмотреть разницу между token-based и bit-based likelihood?).
- можно почитать про prefix trie / prefix array (проект infini-gram (https://infini-gram.io/)). возможно из нее можно имплементации для каверингов переиспользовать?
upd.:
- взять какой-нибудь берт (для начала базовый) и поменять в model_kinds.py (tokenkit) код: добавить туда берт.
- нарисовать схему того что делает чармер. как это происходит в чармере? аналогичным образом можно понять, что нужно сделать мне с помощью генлм. вместо того чтобы получить рандомные символы, мы получаем распределение над символами. 
- разобрать алгоритмы чармера, особенно второй !!!

todo later:
- посмотреть в сторону алгоритмов на автоматах (каверинги от меты (https://github.com/facebookresearch/Exact-Byte-Level-Probabilities-from-Tokenized-LMs/tree/main)).
- ансамблирование побитовых моделей, адверсариальные атаки против ансамбля моделей.

глобально:
- первая цель: не эвристический выбор позиции. 
выбирать позиции для редактирования лучше, чем Charmer и по сути юзать это для ллмок, а не детерминированных энкодеров, но начать лучше с энкодеров (мы хотим не эвристически выбирать позицию, а использовать каверинги). 
посмотреть, могут ли помочь в выборе позиции всякие алгоритмы на автоматах (напр, карасика). надо рассмотреть разные подстроки. нужно сконвертировать модель в последовательность символов, за меньшее количество запросов сделать атаку.
- вторая цель: скорость работы. 
можем ли мы существенно ускорить чармер? (сразу работать с последовательностью битов, быстрее находить вещи, можно еще и хешировать). попытаться сделать это быстрее либо за счет хеширования, либо за счет алгоритмов частичного паттерн-матчинга.

к 31 марта должны быть:
- введение с предварительным вариантом описания исследовательского вопроса, целями и задачами работы
- литобзор по исследуемой проблеме
- основные положения о данных и методологии
- план практической части исследования (либо уже первые результаты практической работы).
также необходимо сузить тему, поменять название (!!!)

по структуре todo:
- начать составлять список литературы: структурировать статьи по тематике, выписать из каждой основные тезисы, чтобы проще было ориентироваться. перечитывая, выписывать цитаты со страницей, которые могли бы пригодиться в ВКР.
